{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5762fa-e6ff-488b-b088-3a5ca69d56a5",
   "metadata": {
    "id": "fd5762fa-e6ff-488b-b088-3a5ca69d56a5"
   },
   "source": [
    "# PyTorch中三个最常见的错误\n",
    "\n",
    "以下是最常见的错误：\n",
    "\n",
    "1. **形状错误** - 您试图对形状不匹配的矩阵/张量执行操作。例如，您的数据形状是 `[1, 28, 28]`，但您的第一层要求输入为 `[10]`。\n",
    "2. **设备错误** - 您的模型位于与您的数据不同的设备上。例如，您的模型位于GPU上（例如 `\"cuda\"`），而您的数据位于CPU上（例如 `\"cpu\"`）。\n",
    "3. **数据类型错误** - 您的数据是一种数据类型（例如 `torch.float32`），但您尝试执行的操作需要另一种数据类型（例如 `torch.int64`）。\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/misc-three-main-errors-in-pytorch.png\" width=750 alt=\"PyTorch中三个最常见的错误\"/>\n",
    "\n",
    "注意这里的重复主题。\n",
    "\n",
    "您的形状、设备和/或数据类型之间存在某种不匹配。\n",
    "\n",
    "本笔记本/博文将介绍上述每种错误的示例以及如何解决它们。\n",
    "\n",
    "这不会阻止您将来犯这些错误，但它会使您足够了解，以减少它们，甚至更重要的是，知道如何解决它们。\n",
    "\n",
    "> **注意：** 以下所有示例都已经改编自 [learnpytorch.io](https://learnpytorch.io)，这是 [从零开始掌握：PyTorch深度学习](https://dbourke.link/ZTMPyTorch) 视频课程的书籍版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0b33b-6a43-4c10-8363-e7b8cac6ceef",
   "metadata": {
    "id": "a1f0b33b-6a43-4c10-8363-e7b8cac6ceef"
   },
   "source": [
    "## 1. PyTorch中张量维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999de2b-88e1-4b7e-a669-928e58b5148d",
   "metadata": {
    "id": "b999de2b-88e1-4b7e-a669-928e58b5148d"
   },
   "source": [
    "### 1.1 矩阵乘法形状错误\n",
    "\n",
    "PyTorch 是构建神经网络模型的最佳框架之一。\n",
    "\n",
    "而神经网络的一个基本操作就是矩阵乘法。\n",
    "\n",
    "然而，矩阵乘法具有非常特定的规则。\n",
    "\n",
    "如果不遵循这些规则，就会出现臭名昭著的形状错误。\n",
    "\n",
    "```\n",
    "RuntimeError: mat1 和 mat2 的形状不能相乘（3x4 和 3x4）\n",
    "```\n",
    "\n",
    "让我们从一个简短的示例开始。\n",
    "\n",
    "> **注意：** 虽然它被称为 \"矩阵乘法\"，但在 PyTorch 中几乎所有形式的数据都以张量的形式出现。张量是一个 n 维数组（n 可以是任意数字）。因此，虽然我使用了术语 \"矩阵乘法\"，但这也适用于 \"张量乘法\"。有关矩阵和张量之间区别的更多信息，请参阅[PyTorch 基础知识：张量简介](https://www.learnpytorch.io/00_pytorch_fundamentals/#introduction-to-tensors)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50eccb-017f-49ac-a274-0cde7d724138",
   "metadata": {
    "id": "9b50eccb-017f-49ac-a274-0cde7d724138",
    "outputId": "fba9f1e2-0805-4e0a-db60-ecd299b73afe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.12.1+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218bcbd-d31d-45b6-8f83-8ccb43a5abf5",
   "metadata": {
    "id": "c218bcbd-d31d-45b6-8f83-8ccb43a5abf5",
    "outputId": "4f930366-d077-4e4c-8b06-082031cd7671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors\n",
    "tensor_1 = torch.rand(3, 4)\n",
    "tensor_2 = torch.rand(3, 4)\n",
    "\n",
    "# Check the shapes\n",
    "print(tensor_1.shape)\n",
    "print(tensor_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827859d9-d392-4a30-8467-39c8a40268fa",
   "metadata": {
    "id": "827859d9-d392-4a30-8467-39c8a40268fa"
   },
   "source": [
    "请注意，两个张量具有相同的形状。\n",
    "\n",
    "让我们尝试对它们执行矩阵乘法。\n",
    "\n",
    "> **注意：** 矩阵乘法操作与标准乘法操作不同。\n",
    ">\n",
    "> 对于我们当前的张量，标准乘法操作（`*` 或 [`torch.mul()`](https://pytorch.org/docs/stable/generated/torch.mul.html)）将起作用，而矩阵乘法操作（`@` 或 [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html)）将出错。\n",
    ">\n",
    "> 请参阅[PyTorch基础知识：矩阵乘法](https://www.learnpytorch.io/00_pytorch_fundamentals/#matrix-multiplication-is-all-you-need)了解矩阵乘法的工作原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90589c0c-1038-4016-b4ea-e51f6c24f6e4",
   "metadata": {
    "id": "90589c0c-1038-4016-b4ea-e51f6c24f6e4",
    "outputId": "066e907f-7c99-40ae-f94a-97c70d4ca529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard multiplication, the following lines perform the same operation (will work)\n",
    "tensor_3 = tensor_1 * tensor_2 # can do standard multiplication with \"*\"\n",
    "tensor_4 = torch.mul(tensor_1, tensor_2) # can also do standard multiplicaton with \"torch.mul()\"\n",
    "\n",
    "# Check for equality\n",
    "tensor_3 == tensor_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d51eacf-87b0-4ea0-becd-feeb95e437ce",
   "metadata": {
    "id": "4d51eacf-87b0-4ea0-becd-feeb95e437ce"
   },
   "source": [
    "太好了！看起来标准乘法在我们当前的张量形状下可以工作。\n",
    "\n",
    "让我们尝试进行矩阵乘法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f996537-6d21-41fd-bcc5-f1c563df34d5",
   "metadata": {
    "id": "6f996537-6d21-41fd-bcc5-f1c563df34d5",
    "outputId": "bc2650df-7a66-47db-d20d-001b8734dfa0"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x4 and 3x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Try matrix multiplication (won't work)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tensor_5 \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_2\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x4 and 3x4)"
     ]
    }
   ],
   "source": [
    "# Try matrix multiplication (won't work)\n",
    "tensor_5 = tensor_1 @ tensor_2 # could also do \"torch.matmul(tensor_1, tensor_2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34574baa-9c23-4e6a-bf45-33ad3da0c7a6",
   "metadata": {
    "id": "34574baa-9c23-4e6a-bf45-33ad3da0c7a6"
   },
   "source": [
    "糟糕！\n",
    "\n",
    "我们遇到了类似以下的错误：\n",
    "\n",
    "```\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-11-2ca2c90dbb42> in <module>\n",
    "      1 # Try matrix multiplication (won't work)\n",
    "----> 2 tensor_5 = tensor_1 @ tensor_2\n",
    "\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (3x4 and 3x4)\n",
    "```\n",
    "\n",
    "这是一个**形状错误**，我们的两个张量（矩阵）不能进行*矩阵*乘法，因为它们的形状不兼容。\n",
    "\n",
    "为什么会这样？\n",
    "\n",
    "这是因为矩阵乘法有特定的规则：\n",
    "\n",
    "1. **内部维度**必须匹配：\n",
    "* `(3, 4) @ (3, 4)` 不起作用\n",
    "* `(4, 3) @ (3, 4)` 起作用\n",
    "* `(3, 4) @ (4, 3)` 起作用\n",
    "2. 结果矩阵的形状是**外部维度**：\n",
    "* `(4, 3) @ (3, 4)` -> `(4, 4)`\n",
    "* `(3, 4) @ (4, 3)` -> `(3, 3)`\n",
    "\n",
    "那么我们该如何解决呢？\n",
    "\n",
    "这就涉及到*转置*或*重塑*操作。\n",
    "\n",
    "在神经网络的情况下，更常见的是转置操作。\n",
    "\n",
    "* **转置** - 转置（[`torch.transpose()`](https://pytorch.org/docs/stable/generated/torch.transpose.html)）操作交换给定张量的维度。\n",
    "  * **注意：** 您还可以使用 `tensor.T` 的快捷方式来执行转置。\n",
    "* **重塑** - 重塑（[`torch.reshape()`](https://pytorch.org/docs/stable/generated/torch.reshape.html)）操作返回一个具有相同原始元素数量但形状不同的张量。\n",
    "\n",
    "让我们看看这些操作的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c1b7f-542b-4163-a094-34ec7f0c4976",
   "metadata": {
    "id": "ba5c1b7f-542b-4163-a094-34ec7f0c4976",
    "outputId": "7552b3d3-ccd7-4eff-ca44-2f3166a419aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensors: torch.Size([4, 3]) and torch.Size([3, 4])\n",
      "Shape of output tensor: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Perform a transpose on tensor_1 and then perform matrix multiplication\n",
    "tensor_6 = tensor_1.T @ tensor_2\n",
    "print(f\"Shape of input tensors: {tensor_1.T.shape} and {tensor_2.shape}\")\n",
    "print(f\"Shape of output tensor: {tensor_6.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145eff9-cfcb-4d91-a732-de7c9b662cf7",
   "metadata": {
    "id": "d145eff9-cfcb-4d91-a732-de7c9b662cf7"
   },
   "source": [
    "没有错误！\n",
    "\n",
    "请注意，由于转置（`tensor_1.T`），`tensor_1` 的输入形状已从 `(3, 4)` 变为 `(4, 3)`。\n",
    "\n",
    "由于这个变化，满足了矩阵乘法的第一条规则，**内部维度必须匹配**。\n",
    "\n",
    "最后，输出形状满足了矩阵乘法的第二条规则，**结果矩阵的形状是外部维度**。\n",
    "\n",
    "在我们的案例中，`tensor_6` 的形状为 `(4, 4)`。\n",
    "\n",
    "让我们执行相同的操作，但这次我们将转置 `tensor_2` 而不是 `tensor_1`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533e23b-0e9d-4bce-aa0c-dae1c2879e65",
   "metadata": {
    "id": "4533e23b-0e9d-4bce-aa0c-dae1c2879e65",
    "outputId": "9cdd24b8-ad9f-43e6-808c-6e49af6144b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensors: torch.Size([3, 4]) and torch.Size([4, 3])\n",
      "Shape of output tensor: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Perform a transpose on tensor_2 and then perform matrix multiplication\n",
    "tensor_7 = tensor_1 @ tensor_2.T\n",
    "print(f\"Shape of input tensors: {tensor_1.shape} and {tensor_2.T.shape}\")\n",
    "print(f\"Shape of output tensor: {tensor_7.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e9014-583a-4db5-ae81-c804c2cfe274",
   "metadata": {
    "id": "d87e9014-583a-4db5-ae81-c804c2cfe274"
   },
   "source": [
    "太棒了！\n",
    "\n",
    "再次没有错误！\n",
    "\n",
    "请注意，矩阵乘法的规则 1 和规则 2 再次得到满足。\n",
    "\n",
    "不同的是，这次由于我们对 `tensor_2` 进行了转置，所以结果输出张量的形状是 `(3, 3)`。\n",
    "\n",
    "好消息是，大多数情况下，当您使用 PyTorch 构建神经网络时，库会为您处理大部分需要执行的矩阵乘法操作。\n",
    "\n",
    "话虽如此，让我们使用 PyTorch 构建一个神经网络，并看看形状错误可能出现的地方。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cca226-4ea2-408e-a05d-52f54cabb62c",
   "metadata": {
    "id": "e2cca226-4ea2-408e-a05d-52f54cabb62c"
   },
   "source": [
    "### 1.2 PyTorch神经网络形状错误\n",
    "\n",
    "我们已经看到了在使用矩阵乘法（或矩阵乘法张量）时形状错误可能发生的情况。\n",
    "\n",
    "现在让我们使用PyTorch构建一个神经网络，并看看形状错误可能发生的地方。\n",
    "\n",
    "在神经网络中，形状错误会在以下任何情况下发生：\n",
    "* **输入形状不正确** - 您的数据处于某种形状，但模型的第一层期望的形状不同。\n",
    "* **层之间的输入和输出形状不正确** - 您的模型的某一层输出某种形状，但下一层期望的输入形状不同。\n",
    "* **在尝试进行预测时输入数据中没有批量大小维度** - 您的模型是在具有批量维度的样本上训练的，因此当您尝试在*没有*批量维度的单个样本上进行预测时，就会出现错误。\n",
    "\n",
    "为了展示这些形状错误，让我们构建一个简单的神经网络（无论您的网络大小如何，错误都是相同的），尝试在时尚MNIST数据集（10种不同类别的黑白服装图像）中找到模式。\n",
    "\n",
    "> **注意：** 以下示例专注于形状错误，而不是构建*最佳*神经网络。您可以在 [PyTorch计算机视觉](https://www.learnpytorch.io/03_pytorch_computer_vision/) 中看到这个问题的完整工作示例。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370b97e-9731-496b-be6a-d2d955dc3bc9",
   "metadata": {
    "id": "c370b97e-9731-496b-be6a-d2d955dc3bc9"
   },
   "source": [
    "### 1.3 下载数据集\n",
    "\n",
    "首先，我们将从 [`torchvision.datasets`](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html) 获取时尚 MNIST 数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932b2c6-49b0-4ce5-80ed-c8118eddd07f",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "966b1ed47e214e189c79c2d1e1c1f2bb",
      "a4d501453aba4c43bf5ea3e9a12ffe80",
      "5a57edd30fc643c0baba05f3595fbe93",
      "f795a425a8d04dd59215f4414997a416"
     ]
    },
    "id": "8932b2c6-49b0-4ce5-80ed-c8118eddd07f",
    "outputId": "48886478-787f-41e4-9cff-85e78dcb58ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966b1ed47e214e189c79c2d1e1c1f2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d501453aba4c43bf5ea3e9a12ffe80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a57edd30fc643c0baba05f3595fbe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f795a425a8d04dd59215f4414997a416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Setup training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=transforms.ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "\n",
    "# Setup testing data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # get test data\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17253b-aa2b-4ad5-bd52-289da2ac934d",
   "metadata": {
    "id": "8a17253b-aa2b-4ad5-bd52-289da2ac934d"
   },
   "source": [
    "现在让我们获取有关第一个训练样本的一些细节，包括标签以及类别名称和类别数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec308d1f-3b54-48c9-9169-3fe25077b651",
   "metadata": {
    "id": "ec308d1f-3b54-48c9-9169-3fe25077b651",
    "outputId": "ae41c194-0230-4ce6-e7c8-3e1cf02a21f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28]) -> [batch, height, width]\n",
      "Label: 9\n"
     ]
    }
   ],
   "source": [
    "# See first training sample\n",
    "image, label = train_data[0]\n",
    "print(f\"Image shape: {image.shape} -> [batch, height, width]\")\n",
    "print(f\"Label: {label}\") # label is an int rather than a tensor (it has no shape attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40897bb1-1c7e-434f-822d-7473e03077a2",
   "metadata": {
    "id": "40897bb1-1c7e-434f-822d-7473e03077a2"
   },
   "source": [
    "我们的图像形状为 `[1, 28, 28]`，或者 `[批量大小, 高度, 宽度]`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf83774-5995-4ddf-8394-d956e25fb85a",
   "metadata": {
    "id": "eaf83774-5995-4ddf-8394-d956e25fb85a",
    "outputId": "f416ff5a-2efd-4097-a476-d033847513cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T-shirt/top',\n",
       "  'Trouser',\n",
       "  'Pullover',\n",
       "  'Dress',\n",
       "  'Coat',\n",
       "  'Sandal',\n",
       "  'Shirt',\n",
       "  'Sneaker',\n",
       "  'Bag',\n",
       "  'Ankle boot'],\n",
       " 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See class names and number of classes\n",
    "class_names = train_data.classes\n",
    "num_classes = len(class_names)\n",
    "class_names, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4b016-5a02-46b9-ba4d-3a3fe144190d",
   "metadata": {
    "id": "85a4b016-5a02-46b9-ba4d-3a3fe144190d",
    "outputId": "1d18ca73-8cfe-4d6b-ea46-09167e6678d7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN1ElEQVR4nO3da2ie9RnH8d9lotUcTFvbrE09RK2d3ZhWPLVaxbPWF0OqVmRMS9e9cC/GBoKMyWQ4V6eDCZswmGN90U3YCwXF0xxssJFW7cQ1jPRFW22bRmPPa9ODPfz3Ik9HFnJfV81jliv6/UCg6c//k/t5nv68k1z879tKKQKQzynjfQAARkY5gaQoJ5AU5QSSopxAUpQTSIpyJmRmxcxmf9oseMylZvb3+o8O/y+UcwyZ2V/NbLeZTRrvYxkrZnaDmfWO93F8HlHOMWJmnZKuk1QkfX18jwYTEeUcOw9IWiNppaQHhwZmttLMnjWzV8xsn5m9ZWYXjvQgZrbQzLaa2Y0jZJPM7OdmtsXM+s3s12Z2hnNMZma/NLO9ZrbezG4eEnSY2UtmtsvMNpjZt4d9nWfMrK/28Uzt75olvSapw8z21z46PtWrhEqUc+w8IOn3tY/bzexLw/L7Jf1Y0hRJGyQ9MfwBzOx2Sc9LuruU8pcRvsbPJM2RNE/SbEmzJP3IOaarJW2SNE3SY5JeMLOptex5Sb2SOiTdI+mnQ8r7Q0nza1/nUklXSXq0lDIgaZGkvlJKS+2jz/n6+DRKKXx8xh+SFko6Imla7fP1kr4/JF8p6bkhn98paf2Qz4ukH0jaLOlrwx67aLCIJmlA0oVDsgWS3q84pqWS+iTZkL97W9I3JZ0j6Zik1iHZCkkra3/eKOnOIdntkj6o/fkGSb3j/Zp/Hj84c46NByX9qZSyo/b5HzTsW1tJHw358wFJLcPy70n6Yymlu+JrTJfUJOkfZrbHzPZIer3291W2lVqjajZr8EzZIWlXKWXfsGxW7c8dtc+Hr8MYahzvA/i8qf3Mt0RSg5mdKOAkSZPN7NJSyj9P8qHulfRbM9tWSnlmhHyHpIOSvlpK2XaSjznLzGxIQc+V9JIGz6hTzax1SEHPlXTicfsknSfpX0OyE9++sq1pjHDm/OzdpcFvEb+iwZ/R5kmaK+lvGvw59GT1SbpZ0nfN7DvDw1LKcUm/kfQLM2uXJDObVfs5tUp77fFONbN7a8f1aillq6QuSSvM7HQzu0TStzT487I0+PPoo2Y23cymafDn2lW1rF/SWWbW9imeG04C5fzsPSjpd6WULaWUj058SPqVpG+Y2Ul/t1JK2aLBgj5iZstH+E8e0eAvk9aY2b8l/VnSl52HfEvSRRo86z4h6Z5Sys5adr+kTg3+T+FFSY+VUt6sZT+RtFbSOkndkt6t/Z1KKes1WN5NtW+v+Xb3M2L/+yMIgCw4cwJJUU4gKcoJJEU5gaTc3xyaGb8tAsZYKcVG+nvOnEBSlBNIinICSVFOICnKCSRFOYGkKCeQFOUEkqKcQFKUE0iKcgJJUU4gKcoJJEU5gaQoJ5AU5QSSopxAUpQTSIpyAklRTiApygkkRTmBpLgFYDJmI14l8b/qvbdNa2urmy9cuLAye+211+r62tFza2hoqMyOHj1a19euV3TsntG+Z5w5gaQoJ5AU5QSSopxAUpQTSIpyAklRTiAp5pzJnHKK///LY8eOufns2bPdfPny5W5+8ODBymxgYMBde+jQITd/++233byeWWY0h4xe12h9PcfmzW89nDmBpCgnkBTlBJKinEBSlBNIinICSVFOICnmnMlEM7FoznnTTTe5+S233OLmvb29ldmkSZPctU1NTW5+6623uvlzzz1XmfX397troz2T0esWaWlpqcyOHz/urj1w4MCoviZnTiApygkkRTmBpCgnkBTlBJKinEBSlBNIijlnMp988kld66+88ko37+zsdHNvzhrtiXzjjTfc/LLLLnPzp556qjJbu3atu7a7u9vNe3p63Pyqq65yc+917erqcteuXr3azatw5gSSopxAUpQTSIpyAklRTiApygkkxShlHHiXYYy2PkXbrq644go337dvn5s3NzdXZnPmzHHXRvk777zj5hs2bKjMvC1bkrRgwQI3X7x4sZsfOXLEzb1jjy43evjwYTevwpkTSIpyAklRTiApygkkRTmBpCgnkBTlBJIyb65mZv7Q7Qsqul1cPaI555o1a9w82hIW8Z5bdBu8ere7ebcQjC4/+e6777q5N0OV4ud2xx13VGYXXHCBu3bWrFluXkoZ8UXnzAkkRTmBpCgnkBTlBJKinEBSlBNIinICSbGfcxSiWeRY2r17t5vPnDnTzQ8ePOjm3m3+Ghv9fy7RnktvjilJZ5xxRmUWzTmvu+46N7/mmmvcPLrsZ3t7e2X2+uuvu2tHizMnkBTlBJKinEBSlBNIinICSVFOICnKCSTFnHOCaWpqcvNoXhflBw4cqMz27t3rrt25c6ebR3tNg73F7troeUWv27Fjx9zcm7Oec8457trR4swJJEU5gaQoJ5AU5QSSopxAUpQTSIpyAkkx5xyFemdu3kwt2hPZ0dHh5tG9IKPc288ZXZfWm5FK0uTJk93cm5NGc8rTTjvNzaP7kra1tbn5unXrKrPoPYvumVqFMyeQFOUEkqKcQFKUE0iKcgJJUU4gKUYpoxBdGrOhocHNvVHKfffd566dMWOGm2/fvt3NvctPSv7WqObmZndttHUqGsV4Y5wjR464a6PLdkbP+6yzznLzZ599tjKbN2+euzY6tiqcOYGkKCeQFOUEkqKcQFKUE0iKcgJJUU4gKQsuRzh+97pLLJpbHT16dNSPffXVV7v5K6+84ubRLf7qmcG2tra6a6Nb/EWXzjz11FNHlUnxDDa6dWLEe25PP/20u3bVqlVuXkoZcQ8iZ04gKcoJJEU5gaQoJ5AU5QSSopxAUpQTSGpM93N6l5CM5m3R5SWjy1N6+/+8PYsno545ZuTVV19184GBATeP5pzRJSS9uXe0VzR6T08//XQ3j/Zs1rM2es+jY7/kkksqs+jWiKPFmRNIinICSVFOICnKCSRFOYGkKCeQFOUEkqprzlnP3sCxnBWOteuvv97N7777bje/9tprK7PoNnrRnshojhntRfXes+jYon8P3nVpJX8OGl0rODq2SPS67d+/vzJbvHixu/bll18e1TFx5gSSopxAUpQTSIpyAklRTiApygkkRTmBpNJet3bq1Klu3tHR4eYXXXTRqNdGc6s5c+a4+eHDh93c26sa7UuM7jPZ19fn5tH1X715X3QPy+j+m01NTW7e1dVVmbW0tLhro9lztJ8z2pPpvW79/f3u2rlz57o5160FJhjKCSRFOYGkKCeQFOUEkqKcQFJ1jVLmz5/vPvjjjz9emU2fPt1dO3nyZDf3tjZJ/valPXv2uGuj7WzRSCAaKXiX9YwubdnT0+PmS5YscfO1a9e6uXebvylTprhrOzs73TyyadOmyiy6/eC+ffvcPNpSFo2ovFHOmWee6a6N/r0wSgEmGMoJJEU5gaQoJ5AU5QSSopxAUpQTSMqdczY2NrpzztWrV7sPPnPmzMosmlNGeT2XQowu4RjNGuvV1tZWmU2bNs1du3TpUje/7bbb3Pyhhx5yc2/L2aFDh9y177//vpt7c0zJ3+ZX73a1aKtcNEf11kfb0c477zw3Z84JTDCUE0iKcgJJUU4gKcoJJEU5gaQoJ5CUO+dctmyZO+d88skn3QffuHFjZRZd6jDKo9vJeaKZlzeHlKStW7e6eXR5Sm8vq3fZTEmaMWOGm991111u7t1mT/L3ZEbvyeWXX15X7j33aI4ZvW7RLf4i3h7c6N9TtO95y5YtzDmBiYRyAklRTiApygkkRTmBpCgnkBTlBJJq9MKPP/7YXRzN+7w9ctFt8qLHjmZu3lwrus7orl273Hzz5s1uHh2bt1802jMZXVP3xRdfdPPu7m439+ac0W0Zo1lkdL1g7/aH0fOO9lRGs8hovTfnjGao0S0jq3DmBJKinEBSlBNIinICSVFOICnKCSTljlK2bdvmLva2m0lSb29vZdbc3OyujS4RGf1afseOHZXZ9u3b3bWNje7LEm5Xi35t723bii7RGG2N8p63JM2dO9fNBwYGKrNovLV79243j14379i9MYsUj1qi9dEtAL2tenv37nXXzps3z82rcOYEkqKcQFKUE0iKcgJJUU4gKcoJJEU5gaTcgd57773nLn7hhRfcfNmyZZVZdPnI6HZx0dYqb9tWNIeMZl7RFqHoFoPedrno1ofRbDm6NeKHH3446sePji2aD9fzntW7Ha2e7WqSP0c9//zz3bX9/f1uXoUzJ5AU5QSSopxAUpQTSIpyAklRTiApygkk5d4C0Mz8oVpg0aJFldnDDz/srm1vb3fzaN+iN9eK5nXRnDKac0bzPu/xvUswSvGcM5rhRrn33KK10bFHvPWjnRWeEL1n0aUxvf2c69atc9cuWbLEzUsp3AIQmEgoJ5AU5QSSopxAUpQTSIpyAklRTiApd87Z0NDgDtWi2VA9brzxRjdfsWKFm3tz0ra2NndtdG3YaA4azTmjOasnui1jNAeNrkXsvaf79+9310avS8Q79mi/ZbSPNXpP33zzTTfv6empzLq6uty1EeacwARDOYGkKCeQFOUEkqKcQFKUE0iKcgJJjel+zqwuvvhiN6/33qBnn322m3/wwQeVWTTP27hxo5tj4mHOCUwwlBNIinICSVFOICnKCSRFOYGkvpCjFCATRinABEM5gaQoJ5AU5QSSopxAUpQTSIpyAklRTiApygkkRTmBpCgnkBTlBJKinEBSlBNIinICSbn7OQGMH86cQFKUE0iKcgJJUU4gKcoJJEU5gaT+A7Hp/CVxPzn3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a sample\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\") # plot image as grayscale\n",
    "plt.axis(False)\n",
    "plt.title(class_names[label]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d8f1db-d8c6-4d2f-9cac-2a19594ad3bb",
   "metadata": {
    "id": "f8d8f1db-d8c6-4d2f-9cac-2a19594ad3bb"
   },
   "source": [
    "### 1.4 构建一系列具有不同形状错误的神经网络\n",
    "\n",
    "我们的问题是：构建一个神经网络，能够在服装的灰度图像中找到模式。\n",
    "\n",
    "这个陈述可以非常深入，因为“哪个神经网络是最好的？”是整个机器学习领域的主要研究问题之一。\n",
    "\n",
    "但是，为了展示不同类型的错误，让我们尽可能简单地开始。\n",
    "\n",
    "我们将使用PyTorch构建几个具有不同错误的两层神经网络，每个网络用于展示不同类型的错误：\n",
    "\n",
    "| **模型编号** | **层次结构** | **错误展示** |\n",
    "| ----- | ----- | ----- |\n",
    "| 0 | 2 x `nn.Linear()`，每层有10个隐藏单元 | 输入形状不正确 |\n",
    "| 1 | 与模型1相同 + 1 x `nn.Flatten()` | 输入形状不正确（仍然） |\n",
    "| 2 | 1 x `nn.Flatten()`，1 x `nn.Linear()`（输入形状正确），1 x `nn.Linear()`，每层有10个隐藏单元 | 无（输入形状正确） |\n",
    "| 3 | 与模型2相同，但在`nn.Linear()`层之间具有不同的形状 | 层之间的形状不正确 |\n",
    "| 4 | 与模型3相同，但最后一层替换为 `nn.LazyLinear()` | 无（展示 `nn.LazyX()` 层如何推断正确的形状） |\n",
    "| 5 | 与模型4相同，但所有 `nn.Linear()` 替换为 `nn.LazyLinear()` | 无（展示 `nn.LazyX()` 层如何推断正确的形状）  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1bf8f-95be-4f80-8701-11a34e7dee3e",
   "metadata": {
    "id": "16a1bf8f-95be-4f80-8701-11a34e7dee3e"
   },
   "source": [
    "### 1.5 错误的网络层输入\n",
    "\n",
    "我们将从一个具有两个层的网络开始，每个层都有10个隐藏单元的 [`nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)。\n",
    "\n",
    "> **注意：** 查看 [01. PyTorch 工作流程第 6 节：将所有内容整合起来](https://www.learnpytorch.io/01_pytorch_workflow/#6-putting-it-all-together) 了解 `nn.Linear()` 内部发生的情况。\n",
    "\n",
    "然后，我们将把我们的 `image` 通过网络传递，并看看会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf7f7a-6100-4d09-9d05-e35d985a773c",
   "metadata": {
    "id": "2aaf7f7a-6100-4d09-9d05-e35d985a773c",
    "outputId": "cd5da120-26ec-4ba9-b86a-9158ad7dd858"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m model_0 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      5\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m      6\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Pass the image through the model (this will error)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Create a two layer neural network\n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=10, out_features=10),\n",
    "    nn.Linear(in_features=10, out_features=10)\n",
    ")\n",
    "\n",
    "# Pass the image through the model (this will error)\n",
    "model_0(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39356d-1e12-40c2-9003-03f268078ed2",
   "metadata": {
    "id": "7e39356d-1e12-40c2-9003-03f268078ed2"
   },
   "source": [
    "运行上述代码，我们会得到另一个形状错误！\n",
    "\n",
    "类似于以下内容：\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112\n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115\n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)\n",
    "```\n",
    "\n",
    "关键在于最后一行 `RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)`。\n",
    "\n",
    "这告诉我们数据形状存在问题。\n",
    "\n",
    "因为在幕后，`nn.Linear()` 正试图进行矩阵乘法。\n",
    "\n",
    "我们如何解决这个问题呢？\n",
    "\n",
    "根据您使用的是哪种类型的层，有几种不同的选择。\n",
    "\n",
    "但由于我们使用的是 `nn.Linear()` 层，让我们专注于这个。\n",
    "\n",
    "`nn.Linear()` 喜欢将数据作为单维向量输入。\n",
    "\n",
    "例如，输入 `image` 的形状不是 `[1, 28, 28]`，而是 `[1, 784]` (`784 = 28*28`) 更合适。\n",
    "\n",
    "换句话说，它希望所有信息都被 *展平* 成一个单维。\n",
    "\n",
    "我们可以使用 PyTorch 的 [`nn.Flatten()`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) 来实现这种展平。\n",
    "\n",
    "让我们看看它是如何发生的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2b1eb-a355-4d87-b895-10a6f3f6ae75",
   "metadata": {
    "id": "3cd2b1eb-a355-4d87-b895-10a6f3f6ae75",
    "outputId": "135bee26-e4a5-45e7-8f6c-733d1ca8a20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before flatten shape: torch.Size([1, 28, 28]) -> [batch, height, width]\n",
      "After flatten shape: torch.Size([1, 784]) -> [batch, height*width]\n"
     ]
    }
   ],
   "source": [
    "# Create a flatten layer\n",
    "flatten = nn.Flatten()\n",
    "\n",
    "# Pass the image through the flatten layer\n",
    "flattened_image = flatten(image)\n",
    "\n",
    "# Print out the image shape before and after\n",
    "print(f\"Before flatten shape: {image.shape} -> [batch, height, width]\")\n",
    "print(f\"After flatten shape: {flattened_image.shape} -> [batch, height*width]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd85d38f-5016-4314-85b1-83e79786fc71",
   "metadata": {
    "id": "fd85d38f-5016-4314-85b1-83e79786fc71"
   },
   "source": [
    "太棒了，图像数据已经展平了！\n",
    "\n",
    "现在让我们尝试将 `nn.Flatten()` 层添加到我们现有的模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54446b26-438e-45b8-96bd-bd3967d35902",
   "metadata": {
    "id": "54446b26-438e-45b8-96bd-bd3967d35902",
    "outputId": "fba24c88-28e8-422c-a286-ec8795a2717d"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model_1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      3\u001b[0m     nn\u001b[38;5;241m.\u001b[39mFlatten(), \u001b[38;5;66;03m# <-- NEW: add nn.Flatten() layer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m      5\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Pass the image through the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)"
     ]
    }
   ],
   "source": [
    "# Replicate model_0 except add a nn.Flatten() layer to begin with\n",
    "model_1 = nn.Sequential(\n",
    "    nn.Flatten(), # <-- NEW: add nn.Flatten() layer\n",
    "    nn.Linear(in_features=10, out_features=10),\n",
    "    nn.Linear(in_features=10, out_features=10)\n",
    ")\n",
    "\n",
    "# Pass the image through the model\n",
    "model_1(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592ac09-b222-4601-bd46-3cb093f81318",
   "metadata": {
    "id": "a592ac09-b222-4601-bd46-3cb093f81318"
   },
   "source": [
    "又出现了另一个错误，类似于：\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112\n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115\n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)\n",
    "```\n",
    "\n",
    "同样，关键信息在底部的行。\n",
    "\n",
    "`RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)`\n",
    "\n",
    "嗯，我们知道 `(1x784)` 必须来自我们的输入数据（`image`），因为我们将其从 `(1, 28, 28)` 展平为 `(1, 784)`。\n",
    "\n",
    "那么 `(10x10)` 呢？\n",
    "\n",
    "这些值来自我们在 `nn.Linear()` 层中设置的参数，即 `in_features=10` 和 `out_features=10` 或 `nn.Linear(in_features=10, out_features=10)`。\n",
    "\n",
    "再次回顾一下矩阵乘法的第一条规则是什么？\n",
    "\n",
    "1. **内部维度**必须匹配。\n",
    "\n",
    "没错！\n",
    "\n",
    "那么如果我们将第一层的 `in_features=10` 改为 `in_features=784` 会发生什么呢？\n",
    "\n",
    "让我们来找找看！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a241a720-8d9a-4308-8ba2-b2b7c92b6240",
   "metadata": {
    "id": "a241a720-8d9a-4308-8ba2-b2b7c92b6240",
    "outputId": "774c3f22-433e-4424-fd71-9bc7d0d1fe43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2045,  0.2677, -0.0713, -0.3096, -0.0586,  0.3153, -0.3413,  0.2031,\n",
       "          0.4421,  0.1715]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the input as well as make sure the first layer can accept the flattened input shape\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=784, out_features=10), # <-- NEW: change in_features=10 to in_features=784\n",
    "    nn.Linear(in_features=10, out_features=10)\n",
    ")\n",
    "\n",
    "# Pass the image through the model\n",
    "model_2(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7bea3-cd05-4c34-9ea0-594429cdf20d",
   "metadata": {
    "id": "d9c7bea3-cd05-4c34-9ea0-594429cdf20d"
   },
   "source": [
    "成功了！\n",
    "\n",
    "我们从模型得到了一个输出！\n",
    "\n",
    "现在输出可能意义不大，但至少我们知道所有的形状都匹配，并且数据可以在我们的模型中流动。\n",
    "\n",
    "`nn.Flatten()` 层将我们的输入图像从 `(1, 28, 28)` 转换为 `(1, 784)`，而我们的第一个 `nn.Linear(in_features=784, out_features=10)` 层可以接受它作为输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161e428-22ce-476d-b19f-5867c77e137e",
   "metadata": {
    "id": "3161e428-22ce-476d-b19f-5867c77e137e"
   },
   "source": [
    "### 1.6 不正确的隐藏层输入和输出层维度\n",
    "\n",
    "如果我们的输入层具有正确的形状，但相互连接的层之间存在不匹配，会发生什么呢？\n",
    "\n",
    "换句话说，我们的第一个 `nn.Linear()` 层具有 `out_features=10`，但接下来的 `nn.Linear()` 层具有 `in_features=5`。\n",
    "\n",
    "这是一个**层之间的输入和输出形状不正确**的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ea9d8-9f7f-4341-b2db-a23649460400",
   "metadata": {
    "id": "ae5ea9d8-9f7f-4341-b2db-a23649460400",
    "outputId": "77fa09d4-1bb8-4e16-b2a0-9aec50b9cce8"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 5x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model_3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      3\u001b[0m     nn\u001b[38;5;241m.\u001b[39mFlatten(),\n\u001b[1;32m      4\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m), \u001b[38;5;66;03m# out_features=10 \u001b[39;00m\n\u001b[1;32m      5\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m# <-- NEW: in_features does not match the out_features of the previous layer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Pass the image through the model (this will error)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 5x10)"
     ]
    }
   ],
   "source": [
    "# Create a model with incorrect input and output shapes between layers\n",
    "model_3 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=784, out_features=10), # out_features=10\n",
    "    nn.Linear(in_features=5, out_features=10) # <-- NEW: in_features does not match the out_features of the previous layer\n",
    ")\n",
    "\n",
    "# Pass the image through the model (this will error)\n",
    "model_3(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb6f61-02ba-4065-b6eb-c920c198816c",
   "metadata": {
    "id": "e5bb6f61-02ba-4065-b6eb-c920c198816c"
   },
   "source": [
    "再次运行上面的模型，我们得到以下错误：\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112\n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115\n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x10 and 5x10)\n",
    "```\n",
    "\n",
    "再次，我们违反了矩阵乘法的第一条规则，**内部维度**必须匹配。\n",
    "\n",
    "我们的第一个 `nn.Linear()` 层输出形状为 `(1, 10)`，但我们的第二个 `nn.Linear()` 层期望的形状为 `(1, 5)`。\n",
    "\n",
    "我们该如何解决这个问题呢？\n",
    "\n",
    "嗯，我们可以手动为第二个 `nn.Linear()` 设置 `in_features=10`，或者我们可以尝试使用 PyTorch 的一项新功能，即“延迟”层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f41b4e-dd31-44f2-a22f-960d8a0d51a2",
   "metadata": {
    "id": "41f41b4e-dd31-44f2-a22f-960d8a0d51a2"
   },
   "source": [
    "### 1.7 PyTorch的延迟层（自动推断输入形状）\n",
    "\n",
    "在PyTorch中，延迟层通常以 `nn.LazyX` 的形式出现，其中 `X` 是该层的现有非延迟形式。\n",
    "\n",
    "例如，`nn.Linear()` 的延迟等价物是 [`nn.LazyLinear()`](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html)。\n",
    "\n",
    "延迟层的主要特征是*推断*前一层的 `in_features` 或输入形状应该是什么。\n",
    "\n",
    "> **注意：** 截至2022年11月，PyTorch中的延迟层仍处于实验阶段，可能会发生变化，但它们的使用方式不应与以下内容有太大差异。\n",
    "\n",
    "例如，如果前一层具有 `out_features=10`，则随后的延迟层应该推断 `in_features=10`。\n",
    "\n",
    "让我们来测试一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615051b-93ac-4dd9-a480-8e24619aeaa3",
   "metadata": {
    "id": "e615051b-93ac-4dd9-a480-8e24619aeaa3",
    "outputId": "279969a3-298d-4170-87ce-2b006532beed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4282,  0.2492, -0.2045, -0.4943, -0.1639,  0.1166,  0.3828, -0.1283,\n",
       "         -0.1771, -0.2277]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try nn.LazyLinear() as the second layer\n",
    "model_4 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=784, out_features=10),\n",
    "    nn.LazyLinear(out_features=10) # <-- NEW: no in_features parameter as this is inferred from the previous layer's output\n",
    ")\n",
    "\n",
    "# Pass the image through the model\n",
    "model_4(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d421bf-e58d-4a5d-b1db-19616dfe9f65",
   "metadata": {
    "id": "a7d421bf-e58d-4a5d-b1db-19616dfe9f65"
   },
   "source": [
    "它可以工作（尽管根据您使用的PyTorch版本可能会有警告，如果有的话，不要担心，这只是说 `Lazy` 层仍在开发中）！\n",
    "\n",
    "我们试试用 `nn.LazyLinear()` 层替换所有的 `nn.Linear()` 层吧？\n",
    "\n",
    "然后我们只需要为每个层设置 `out_features` 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2230de11-f765-495b-8e9f-0c680d9902f6",
   "metadata": {
    "id": "2230de11-f765-495b-8e9f-0c680d9902f6",
    "outputId": "a7ccdb18-0685-4587-ee9f-7a0c1b0f9497"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1375, -0.2175, -0.1054,  0.1424, -0.1406, -0.1180, -0.0896, -0.4285,\n",
       "         -0.0077, -0.3188]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all nn.Linear() layers with nn.LazyLinear()\n",
    "model_5 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.LazyLinear(out_features=10),\n",
    "    nn.LazyLinear(out_features=10) # <-- NEW\n",
    ")\n",
    "\n",
    "# Pass the image through the model\n",
    "model_5(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1cdcbc-0f57-4371-b753-655f36bbea3a",
   "metadata": {
    "id": "3e1cdcbc-0f57-4371-b753-655f36bbea3a"
   },
   "source": [
    "太棒了！又成功了，我们的图像能够顺利通过网络流动，没有任何问题。\n",
    "\n",
    "> **注意：** 上面的例子只涉及PyTorch中一种类型的层，即 `nn.Linear()`，然而，将输入和输出形状与每个层对齐的原则在所有神经网络和不同类型的数据中都是一致的。\n",
    ">\n",
    "> 像 [`nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) 这样的层，用于卷积神经网络（CNNs），甚至可以在不使用 `nn.Flatten()` 的情况下接受输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb1210-564f-404b-b5d3-a149972cc22b",
   "metadata": {
    "id": "14bb1210-564f-404b-b5d3-a149972cc22b"
   },
   "source": [
    "## 2. PyTorch中的设备错误\n",
    "\n",
    "PyTorch的一个主要优势是其内置的在GPU（图形处理单元）上进行计算的能力。\n",
    "\n",
    "GPU通常能够比CPU（中央处理单元）更快地执行操作，特别是矩阵乘法（构成神经网络大部分操作）。\n",
    "\n",
    "如果您使用的是原始的PyTorch（没有其他外部库），PyTorch要求您明确设置您正在计算的设备。\n",
    "\n",
    "例如，要将您的模型发送到目标 `device`，您会使用 [`to()`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) 方法，如 `model.to(device)`。\n",
    "\n",
    "数据也是类似的，使用 `some_dataset.to(device)`。\n",
    "\n",
    "**设备错误**发生在您的模型/数据位于不同设备上时。\n",
    "\n",
    "例如，当您将模型发送到目标GPU设备，但您的数据仍然在CPU上时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ade20-6f1b-4fc6-944d-553f2112834d",
   "metadata": {
    "id": "ab9ade20-6f1b-4fc6-944d-553f2112834d"
   },
   "source": [
    "### 2.1 设置目标设备\n",
    "\n",
    "如果可用，让我们将当前设备设置为 `\"cuda\"`。\n",
    "\n",
    "> **注意：** 更多关于如何访问GPU并在PyTorch中设置它的信息，请参阅 [00. PyTorch 基础知识：在GPU上运行张量](https://www.learnpytorch.io/00_pytorch_fundamentals/#running-tensors-on-gpus-and-making-faster-computations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff03038-c2f0-4e51-a46e-9d1e8fbde893",
   "metadata": {
    "id": "aff03038-c2f0-4e51-a46e-9d1e8fbde893",
    "outputId": "fe350168-c70a-4144-c523-7ee719f39ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set device to \"cuda\" if it's available otherwise default to \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Current device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494deffc-46a7-49e1-8a52-a73636982162",
   "metadata": {
    "id": "494deffc-46a7-49e1-8a52-a73636982162"
   },
   "source": [
    "现在让我们创建一个与 `model_5` 相同层的模型。\n",
    "\n",
    "在PyTorch中，默认情况下，模型和张量是在CPU上创建的。\n",
    "\n",
    "我们可以通过检查我们创建的模型的 `device` 属性来测试这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff50c25-5e81-43d6-816d-8816c58cbb02",
   "metadata": {
    "id": "cff50c25-5e81-43d6-816d-8816c58cbb02",
    "outputId": "dcae5a9c-7f24-45c2-ff77-f93cff0e8174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Create a model (similar to model_5 above)\n",
    "model_6 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.LazyLinear(out_features=10),\n",
    "    nn.LazyLinear(out_features=10)\n",
    ")\n",
    "\n",
    "# All models and tensors are created on the CPU by default (unless explicitly set otherwise)\n",
    "print(f\"Model is on device: {next(model_6.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c0e5f9-fcc1-4abf-a81d-496aa2f8b5cc",
   "metadata": {
    "id": "17c0e5f9-fcc1-4abf-a81d-496aa2f8b5cc"
   },
   "source": [
    "### 2.2 为模型准备数据\n",
    "\n",
    "为了为建模做好数据准备，让我们创建一些PyTorch的`DataLoader`。\n",
    "\n",
    "为了加快速度，我们将使用[`torch.utils.data.RandomSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler)的一个实例来随机选择训练和测试样本的10%（我们更关心展示潜在的错误，而不是最佳性能模型）。\n",
    "\n",
    "我们还将设置损失函数为`torch.nn.CrossEntropyLoss()`，以及优化器为`torch.optim.SGD(lr=0.01)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3873224-cf4e-4ec9-aa2d-713166ace3cb",
   "metadata": {
    "id": "b3873224-cf4e-4ec9-aa2d-713166ace3cb",
    "outputId": "b55e34b4-4dbd-4491-9a07-a49b73475114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of random training samples selected: 6000/60000\n",
      "Number of random testing samples selected: 1000/10000\n",
      "Number of batches in train_dataloader: 188 batches of size 32\n",
      "Number of batches in test_dataloader: 32 batch of size 32\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "# Only sample 10% of the data\n",
    "train_sampler = RandomSampler(train_data,\n",
    "                              num_samples=int(0.1*len(train_data)))\n",
    "\n",
    "test_sampler = RandomSampler(test_data,\n",
    "                             num_samples=int(0.1*len(test_data)))\n",
    "\n",
    "print(f\"Number of random training samples selected: {len(train_sampler)}/{len(train_data)}\")\n",
    "print(f\"Number of random testing samples selected: {len(test_sampler)}/{len(test_data)}\")\n",
    "\n",
    "# Create DataLoaders and turn data into batches\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              sampler=train_sampler)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             sampler=test_sampler)\n",
    "\n",
    "print(f\"Number of batches in train_dataloader: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\n",
    "print(f\"Number of batches in test_dataloader: {len(test_dataloader)} batch of size {BATCH_SIZE}\")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(lr=0.01,\n",
    "                            params=model_6.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d9fb5e-0ff8-4869-ba04-b62bab59f02c",
   "metadata": {
    "id": "23d9fb5e-0ff8-4869-ba04-b62bab59f02c"
   },
   "source": [
    "### 2.3 在CPU上训练模型\n",
    "\n",
    "数据准备好了，模型也准备好了，让我们开始训练吧！\n",
    "\n",
    "我们将使用标准的PyTorch训练循环，在10%的数据上对 `model_6` 进行五个周期的训练。\n",
    "\n",
    "在这里不要太担心损失是否尽可能低，因为我们更关注的是确保没有任何错误，而不是损失尽可能低。\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/01-pytorch-training-loop-annotated.png\" alt=\"annotated pytorch training loop steps\" width=750/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6f92e-d4aa-40bf-b3ed-c97a98c72e3d",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "28ec173850c84277a11540fc3cd0350f"
     ]
    },
    "id": "03d6f92e-d4aa-40bf-b3ed-c97a98c72e3d",
    "outputId": "826faad9-cf3b-4193-b800-e28ba1f0af8f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ec173850c84277a11540fc3cd0350f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 334.65\n",
      "Epoch: 1 | Training loss: 215.44\n",
      "Epoch: 2 | Training loss: 171.15\n",
      "Epoch: 3 | Training loss: 154.72\n",
      "Epoch: 4 | Training loss: 142.22\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # Set loss to 0 every epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    # Get images (X) and labels (y)\n",
    "    for X, y in train_dataloader:\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model_6(X)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss in the epoch loop only\n",
    "    print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d3079-a360-44fb-a955-05f7f74edaec",
   "metadata": {
    "id": "884d3079-a360-44fb-a955-05f7f74edaec"
   },
   "source": [
    "太好了！看起来我们的训练循环正在运行！\n",
    "\n",
    "我们模型的损失在下降（损失越低越好）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f79817-30ff-4f06-a201-954acba34abb",
   "metadata": {
    "id": "c5f79817-30ff-4f06-a201-954acba34abb"
   },
   "source": [
    "### 2.4 尝试在GPU上训练模型\n",
    "\n",
    "现在让我们将 `model_6` 发送到目标 `device`（在我们的情况下，这是一个 `\"cuda\"` GPU）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce89bb-2624-4427-9de1-7570154bb8ee",
   "metadata": {
    "id": "e4ce89bb-2624-4427-9de1-7570154bb8ee",
    "outputId": "2ce4523e-9c56-48cd-a4b0-53a5d1b3940b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Send model_6 to the target device (\"cuda\")\n",
    "model_6.to(device)\n",
    "\n",
    "# Print out what device the model is on\n",
    "print(f\"Model is on device: {next(model_6.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c9434-9a9a-4089-98fd-32196036504b",
   "metadata": {
    "id": "e75c9434-9a9a-4089-98fd-32196036504b"
   },
   "source": [
    "我们的 `model_6` 在 `\"cuda:0\"` 设备上（其中 `0` 是设备的索引，如果有多个GPU的话）。\n",
    "\n",
    "现在让我们运行上面相同的训练循环代码，看看会发生什么。\n",
    "\n",
    "你能猜到吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7552567-8d35-4570-8dd9-f984d02cd2e8",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "75606965bc224a6a8b223db95612e24e"
     ]
    },
    "id": "a7552567-8d35-4570-8dd9-f984d02cd2e8",
    "outputId": "7c930f72-2bd4-429f-d181-fd6d0bfe8656"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75606965bc224a6a8b223db95612e24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Get images (X) and labels (y)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m   \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# model is on GPU, data is on CPU (will error)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m     19\u001b[0m   loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "  # Set loss to 0 every epoch\n",
    "  train_loss = 0\n",
    "\n",
    "  # Get images (X) and labels (y)\n",
    "  for X, y in train_dataloader:\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model_6(X) # model is on GPU, data is on CPU (will error)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss\n",
    "\n",
    "    # Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "  # Print loss in the epoch loop only\n",
    "  print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dde9a2-8c87-44d2-9d2c-3257589e37c3",
   "metadata": {
    "id": "c6dde9a2-8c87-44d2-9d2c-3257589e37c3"
   },
   "source": [
    "糟糕！\n",
    "\n",
    "看起来我们遇到了一个设备错误：\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112\n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115\n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n",
    "```\n",
    "\n",
    "我们可以看到错误信息中指出 `Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!`。\n",
    "\n",
    "实质上，我们的模型在 `cuda:0` 设备上，但我们的数据张量（`X` 和 `y`）仍在 `cpu` 设备上。\n",
    "\n",
    "但是 **PyTorch 期望 *所有* 张量都在同一设备上**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c50ba5-d2ae-4b1b-98f2-5b8c9b0d366a",
   "metadata": {
    "id": "c8c50ba5-d2ae-4b1b-98f2-5b8c9b0d366a"
   },
   "source": [
    "### 2.5 在GPU上训练模型\n",
    "\n",
    "让我们通过将数据张量（`X` 和 `y`）发送到目标 `device` 来修复此错误。\n",
    "\n",
    "我们可以使用 `X.to(device)` 和 `y.to(device)` 来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b45f9-abd3-4abe-bd34-17052312dee5",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "3f94f47483b84c0ca344c1c3545a9d04"
     ]
    },
    "id": "505b45f9-abd3-4abe-bd34-17052312dee5",
    "outputId": "9ea03737-6251-4a33-91a1-6bb46886072d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f94f47483b84c0ca344c1c3545a9d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 134.76\n",
      "Epoch: 1 | Training loss: 127.76\n",
      "Epoch: 2 | Training loss: 120.85\n",
      "Epoch: 3 | Training loss: 120.50\n",
      "Epoch: 4 | Training loss: 116.29\n"
     ]
    }
   ],
   "source": [
    "# Send the model to the target device (we don't need to do this again but we will for completeness)\n",
    "model_6.to(device)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "  # Set loss to 0 every epoch\n",
    "  train_loss = 0\n",
    "\n",
    "  # Get images (X) and labels (y)\n",
    "  for X, y in train_dataloader:\n",
    "\n",
    "    # Put target data on target device  <-- NEW\n",
    "    X, y = X.to(device), y.to(device) # <-- NEW: send data to target device\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model_6(X)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss\n",
    "\n",
    "    # Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "  # Print loss in the epoch loop only\n",
    "  print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4866ae09-785a-4ddb-b2f3-a9e224bf7929",
   "metadata": {
    "id": "4866ae09-785a-4ddb-b2f3-a9e224bf7929"
   },
   "source": [
    "太棒了！\n",
    "\n",
    "我们的训练循环完成了，就像以前一样，因为现在我们的模型 *和* 数据张量都在同一个设备上。\n",
    "\n",
    "> **注意：** 像 [HuggingFace Accelerate](https://github.com/huggingface/accelerate) 这样的库是训练PyTorch模型的绝佳选择，几乎不需要显式设置设备（它们会自动发现最佳设备并为您设置好）。\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b8ac3-192e-4299-8545-a2fd389c36a0",
   "metadata": {
    "id": "098b8ac3-192e-4299-8545-a2fd389c36a0"
   },
   "source": [
    "### 2.6 预测时的设备错误\n",
    "\n",
    "我们在训练过程中看到了设备错误，但同样的错误也可能在测试或推理（进行预测）过程中发生。\n",
    "\n",
    "训练模型的整个想法是使用它对*未见过*的数据进行预测。\n",
    "\n",
    "让我们拿我们训练好的 `model_6`，并在测试数据集中的一个样本上使用它进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38082d32-4d43-43e9-974e-f00f4f2724fa",
   "metadata": {
    "id": "38082d32-4d43-43e9-974e-f00f4f2724fa",
    "outputId": "9eb41d27-9e5d-43ce-9a5e-a0475ba903f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image shape: torch.Size([28, 28])\n",
      "Test image label: 9\n"
     ]
    }
   ],
   "source": [
    "# Get a single sample from the test dataset\n",
    "test_image, test_label = test_data.data[0], test_data.targets[0]\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "print(f\"Test image label: {test_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a5ebb-058b-4b3f-ba0f-ccf041a580c6",
   "metadata": {
    "id": "e64a5ebb-058b-4b3f-ba0f-ccf041a580c6",
    "outputId": "9de8284e-b96b-4ee8-aa88-0f9f77318b68"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot test image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(test_image, cmap=\"gray\")\n",
    "plt.axis(False)\n",
    "plt.title(class_names[test_label]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f98c15-6b4a-457a-a7c9-bfe62692c97c",
   "metadata": {
    "id": "90f98c15-6b4a-457a-a7c9-bfe62692c97c"
   },
   "source": [
    "看起来不错！\n",
    "\n",
    "现在让我们尝试通过将其传递给我们的 `model_6` 来对其进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f77c721-c9f5-4f65-bacb-899a06e72148",
   "metadata": {
    "id": "4f77c721-c9f5-4f65-bacb-899a06e72148",
    "outputId": "babce265-2dfd-4e69-8064-e56116f3605e"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Pass the test image through model_6 to make a prediction\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "# Pass the test image through model_6 to make a prediction\n",
    "model_6(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aeb66d-e84d-4b95-b1ed-052a023a50d5",
   "metadata": {
    "id": "79aeb66d-e84d-4b95-b1ed-052a023a50d5"
   },
   "source": [
    "糟糕！\n",
    "\n",
    "我们又遇到了另一个设备错误。\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112\n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115\n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n",
    "```\n",
    "\n",
    "这是因为我们的 `model_6` 在GPU上（`\"cuda\"`），然而，我们的 `test_image` 在CPU上（在PyTorch中，默认情况下所有张量都在CPU上）。\n",
    "\n",
    "让我们将 `test_image` 发送到目标 `device`，然后再尝试进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3d722-24e5-4df0-907d-4553050fd2f9",
   "metadata": {
    "id": "b1a3d722-24e5-4df0-907d-4553050fd2f9",
    "outputId": "e51ce138-29b8-4953-bc3e-b6a4f00bac75"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28x28 and 784x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Send test_image to target device\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x10)"
     ]
    }
   ],
   "source": [
    "# Send test_image to target device\n",
    "model_6(test_image.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfecaf-d7b2-49ba-a98f-016d4078bf36",
   "metadata": {
    "id": "1ccfecaf-d7b2-49ba-a98f-016d4078bf36"
   },
   "source": [
    "哦，不！又是另一个错误...\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112\n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115\n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x10)\n",
    "```\n",
    "\n",
    "这次是形状错误。\n",
    "\n",
    "我们以前见过这些。\n",
    "\n",
    "我们的 `test_image` 形状出了什么问题？\n",
    "\n",
    "也许是因为我们的模型是在带有批次维度的图像上训练的？\n",
    "\n",
    "而我们当前的 `test_image` 没有批次维度？\n",
    "\n",
    "这里有另一个有用的经验法则要记住：**训练过的模型喜欢以与它们训练时相同的格式和形状的数据进行预测**。\n",
    "\n",
    "这意味着如果我们的模型是在带有批次维度的图像上训练的，即使批次维度只有1（单个样本），它也倾向于喜欢对带有批次维度的图像进行预测。\n",
    "\n",
    "如果我们的模型是在格式为 `torch.float32`（或其他格式）的数据上进行训练的，它也喜欢对具有相同格式的数据进行预测（我们稍后会看到这一点）。\n",
    "\n",
    "我们可以使用 [`torch.unsqueeze()`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) 方法向我们的 `test_image` 添加一个单独的批次维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc1ba7-9156-44ee-b6b3-35771106a1a5",
   "metadata": {
    "id": "e8dc1ba7-9156-44ee-b6b3-35771106a1a5",
    "outputId": "ea08e27e-4c33-40c4-bfd2-121f3b5f6d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input data shape: torch.Size([28, 28]) -> [height, width]\n",
      "Updated input data shape (with added batch dimension): torch.Size([1, 28, 28]) -> [batch, height, width]\n"
     ]
    }
   ],
   "source": [
    "# Changing the input size to be the same as what the model was trained on\n",
    "original_input_shape = test_image.shape\n",
    "updated_input_shape = test_image.unsqueeze(dim=0).shape # adding a batch dimension on the \"0th\" dimension\n",
    "\n",
    "# Print out shapes of original tensor and updated tensor\n",
    "print(f\"Original input data shape: {original_input_shape} -> [height, width]\")\n",
    "print(f\"Updated input data shape (with added batch dimension): {updated_input_shape} -> [batch, height, width]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745679ec-251e-4cb3-9732-d9954dbe0e2c",
   "metadata": {
    "id": "745679ec-251e-4cb3-9732-d9954dbe0e2c"
   },
   "source": [
    "太好了！\n",
    "\n",
    "我们找到了一种方法为我们的 `test_image` 添加一个批次维度。\n",
    "\n",
    "让我们再次尝试对其进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25cef5-e0db-44dd-bbc7-17ff104ab846",
   "metadata": {
    "id": "3a25cef5-e0db-44dd-bbc7-17ff104ab846",
    "outputId": "09854f64-2353-4ea9-8179-323019c966f1"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make prediction on test image with additional batch size dimension and with it on the target device\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Byte"
     ]
    }
   ],
   "source": [
    "# Make prediction on test image with additional batch size dimension and with it on the target device\n",
    "model_6(test_image.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a30c81-4118-471e-8672-7897257e8709",
   "metadata": {
    "id": "49a30c81-4118-471e-8672-7897257e8709"
   },
   "source": [
    "什么？\n",
    "\n",
    "又是另一个错误！\n",
    "\n",
    "这次是一个数据类型错误：\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112\n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115\n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: expected scalar type Float but found Byte\n",
    "```\n",
    "\n",
    "我们碰到了 PyTorch 中第三个最常见的错误，数据类型错误。\n",
    "\n",
    "让我们在下一节中找出如何修复它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053b033-862b-4c67-b2e4-dc8de4d0fd6e",
   "metadata": {
    "id": "4053b033-862b-4c67-b2e4-dc8de4d0fd6e"
   },
   "source": [
    "## 3. PyTorch中的设备错误\n",
    "\n",
    "回想一下经验法则：**训练过的模型喜欢对与它们训练时相同形状和格式的数据进行预测**。\n",
    "\n",
    "看起来我们的模型期望的是 `Float` 数据类型，但我们的 `test_image` 是 `Byte` 数据类型。\n",
    "\n",
    "我们可以通过前一个错误中的最后一行来确定：\n",
    "\n",
    "```\n",
    "RuntimeError: expected scalar type Float but found Byte\n",
    "```\n",
    "\n",
    "为什么会这样呢？\n",
    "\n",
    "因为我们的 `model_6` 是在 `Float` 格式的数据样本上进行训练的，具体来说是 `torch.float32`。\n",
    "\n",
    "我们如何知道这一点呢？\n",
    "\n",
    "嗯，`torch.float32` 是 PyTorch 中许多张量的默认值，除非另有明确设置。\n",
    "\n",
    "但让我们进行一次检查以确保。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa086a9-d8ef-4438-9e86-512cfd86dd6e",
   "metadata": {
    "id": "7fa086a9-d8ef-4438-9e86-512cfd86dd6e"
   },
   "source": [
    "### 3.1 检查训练模型的数据类型\n",
    "\n",
    "我们可以通过查看来自 `train_dataloader` 的样本的 `dtype` 属性来检查我们的模型训练所使用的数据的数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9145327-5d47-48fa-a6c1-5763896bed90",
   "metadata": {
    "id": "f9145327-5d47-48fa-a6c1-5763896bed90",
    "outputId": "76ee6274-9385-45fa-9c37-321b93427565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of training data: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Get a single sample from the train_dataloader and print the dtype\n",
    "train_image_batch, train_label_batch = next(iter(train_dataloader))\n",
    "train_image_single, train_label_single = train_image_batch[0], train_label_batch[0]\n",
    "\n",
    "# Print the datatype of the train_image_single\n",
    "print(f\"Datatype of training data: {train_image_single.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d881ad2-8aeb-4742-8823-8dbaab7f4e18",
   "metadata": {
    "id": "0d881ad2-8aeb-4742-8823-8dbaab7f4e18"
   },
   "source": [
    "搞定！我们确认了我们的训练数据样本是 `torch.float32` 类型。\n",
    "\n",
    "所以，我们的 `model_6` 想要对这种数据类型进行预测是合理的。\n",
    "\n",
    "但是我们的训练数据是如何变成这种数据类型的呢？\n",
    "\n",
    "这是发生在第 1.3 节中的，当我们下载了 Fashion MNIST 数据集并使用了 [`torchvision.transforms.ToTensor()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html) 的 `transform` 参数时发生的。\n",
    "\n",
    "这个 `transform` 将传递给它的任何数据转换为一个 `torch.Tensor`，其默认数据类型为 `torch.float32`。\n",
    "\n",
    "因此，另一个经验法则是：**在进行预测时，对训练数据执行的任何转换都应该对测试数据执行相同的转换**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d2a74-4c28-4c9a-a9ad-05cc81c71a14",
   "metadata": {
    "id": "4f8d2a74-4c28-4c9a-a9ad-05cc81c71a14"
   },
   "source": [
    "### 3.2 改变张量数据类型\n",
    "\n",
    "在我们的情况下，我们可以创建一个独立的转换来转换我们的测试数据，但我们也可以使用 `tensor.type(some_type_here)` 来更改目标张量的数据类型，例如，`tensor_1.type(torch.float32)`。\n",
    "\n",
    "让我们试一试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e031504-824a-4ea1-b159-dbf54846af26",
   "metadata": {
    "id": "9e031504-824a-4ea1-b159-dbf54846af26",
    "outputId": "7c784dde-0242-4135-e6a3-8c7916d0f071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original datatype: torch.uint8\n",
      "Changing the datatype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Print out the original datatype of test_image\n",
    "print(f\"Original datatype: {test_image.unsqueeze(dim=0).dtype}\")\n",
    "\n",
    "# Change the datatype of test_image and see the change\n",
    "print(f\"Changing the datatype: {test_image.unsqueeze(dim=0).type(torch.float32).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a01303-8a2f-40cf-8a39-077acc4063cd",
   "metadata": {
    "id": "d2a01303-8a2f-40cf-8a39-077acc4063cd"
   },
   "source": [
    "### 3.3 对测试图像进行预测，并确保其格式正确\n",
    "\n",
    "好的，看起来我们已经准备好了所有的拼图，形状、设备和数据类型，让我们尝试进行预测吧！\n",
    "\n",
    "> **注意：**记住，模型喜欢对与其训练时相同（或类似）格式的数据进行预测（形状、设备和数据类型）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db9993-0148-4881-9f68-fcea63946c13",
   "metadata": {
    "id": "c2db9993-0148-4881-9f68-fcea63946c13",
    "outputId": "294c1f4f-24dd-4ee4-9a79-99a1d3e0882c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -963.8352, -1658.8182,  -735.9952, -1285.2964,  -550.3845,   949.4190,\n",
       "          -538.1960,  1123.0616,   552.7371,  1413.8110]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a prediction with model_6 on the transformed test_image\n",
    "pred_on_gpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension\n",
    "                      .type(torch.float32) # convert the datatype to torch.float32\n",
    "                      .to(device)) # send the tensor to the target device\n",
    "pred_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef441646-1d24-4fde-9fa5-0b54ce9de2fe",
   "metadata": {
    "id": "ef441646-1d24-4fde-9fa5-0b54ce9de2fe"
   },
   "source": [
    "哇哦！\n",
    "\n",
    "经过一番努力，我们的 `model_6` 成功对 `test_image` 进行了预测。\n",
    "\n",
    "由于 `test_image` 默认位于 CPU 上，我们也可以使用 [`.cpu()` 方法](https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html) 将模型放回到 CPU 上，然后在 CPU 设备上进行相同的预测，而不是在 GPU 设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bd335-43ca-4c76-ac66-f3ffccf79dc1",
   "metadata": {
    "id": "e98bd335-43ca-4c76-ac66-f3ffccf79dc1",
    "outputId": "e844abdd-ac73-4cd5-9de0-5e5756c04b7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -963.8351, -1658.8182,  -735.9953, -1285.2964,  -550.3845,   949.4189,\n",
       "          -538.1960,  1123.0615,   552.7371,  1413.8110]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put model back on CPU\n",
    "model_6.cpu()\n",
    "\n",
    "# Make a prediction on the CPU device (no need to put test_image on the CPU as it's already there)\n",
    "pred_on_cpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension\n",
    "                      .type(torch.float32)) # convert the datatype to torch.float32\n",
    "pred_on_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2269862-f9d9-4985-9483-1821132b8605",
   "metadata": {
    "id": "d2269862-f9d9-4985-9483-1821132b8605"
   },
   "source": [
    "再次，预测成功了！\n",
    "\n",
    "那么它是正确的吗？\n",
    "\n",
    "我们可以通过将模型的原始输出从 `原始对数 -> 预测概率 -> 预测标签` 进行转换来检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbaa8ae-3954-43d4-92e1-3a0074428056",
   "metadata": {
    "id": "afbaa8ae-3954-43d4-92e1-3a0074428056",
    "outputId": "4277911e-a83a-47b4-91cb-45a53cb87e7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label: 9\n",
      "Pred label: tensor([9])\n",
      "Is the prediction correct? True\n"
     ]
    }
   ],
   "source": [
    "# Convert raw logits to prediction probabilities\n",
    "pred_probs = torch.softmax(pred_on_cpu, dim=1)\n",
    "\n",
    "# Convert prediction probabilities to prediction label\n",
    "pred_label = torch.argmax(pred_probs, dim=1)\n",
    "\n",
    "# Check if it's correct\n",
    "print(f\"Test label: {test_label}\")\n",
    "print(f\"Pred label: {pred_label}\")\n",
    "print(f\"Is the prediction correct? {pred_label.item() == test_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a3073-6c98-4137-aa3b-f2df832d1a67",
   "metadata": {
    "id": "3f8a3073-6c98-4137-aa3b-f2df832d1a67"
   },
   "source": [
    "在对测试或自定义样本进行预测时，可能涉及到很多步骤。\n",
    "\n",
    "因此，防止重复所有这些步骤的一种方法是将它们转换为一个函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740aaa9-e3a7-42ea-9ca0-d4dc5be67766",
   "metadata": {
    "id": "2740aaa9-e3a7-42ea-9ca0-d4dc5be67766"
   },
   "source": [
    "## 将所有内容整合起来\n",
    "\n",
    "我们已经亲自体验了在使用PyTorch构建神经网络时会遇到的三个主要错误：\n",
    "\n",
    "1. **形状错误** - 数据与正在构建的神经网络之间存在不匹配，或者神经网络中连接的层之间存在不匹配。\n",
    "2. **设备错误** - 您的模型和数据位于不同的设备上，PyTorch希望*所有*张量和对象都位于*相同*的设备上。\n",
    "3. **数据类型错误** - 您试图在一个数据类型上进行计算，而模型期望另一种数据类型。\n",
    "\n",
    "我们已经了解了它们是如何发生以及为什么会发生，然后学习了如何修复它们：\n",
    "\n",
    "* 您的模型希望对与其训练相同类型的数据进行预测（形状、设备和数据类型）。\n",
    "* 您的模型和数据应在训练和测试中处于同一设备上。\n",
    "* 您可以通过创建可重用函数来解决许多这些问题，该函数定义了`device`和数据类型，例如[04. PyTorch 模块化部分 4：创建训练和测试函数](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them)中所述。\n",
    "\n",
    "了解这些错误不会阻止您将来出现这些错误，但它将为您提供修复它们的思路。\n",
    "\n",
    "有关这些错误的更深入示例，包括如何在实践中制造并修复它们，请查看[从零开始：深度学习PyTorch课程](https://dbourke.link/ZTMPyTorch)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba6bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
